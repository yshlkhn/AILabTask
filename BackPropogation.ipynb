{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Back Propogation***"
      ],
      "metadata": {
        "id": "q9SUdsSh5Fbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "nBOhpsMm5HDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = 0.03\n",
        "x2 = 0.23\n"
      ],
      "metadata": {
        "id": "Jl-wFurg5aZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sigmoid activation***"
      ],
      "metadata": {
        "id": "Xk_cpFsWABO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial weights (input -> hidden)\n",
        "w1, w2 = 0.02, 0.20  # weights to H1 from x1, x2\n",
        "w3, w4 = 0.25, 0.70  # weights to H2 from x1, x2\n",
        "\n",
        "# Initial weights (hidden -> output)\n",
        "w5, w6 = 0.30, 0.45  # weights to y1 from H1, H2\n",
        "w7, w8 = 0.56, 0.55  # weights to y2 from H1, H2\n",
        "\n",
        "# Biases\n",
        "b1 = 0.10  # bias for hidden neurons H1, H2 (same in example)\n",
        "b2 = 0.09  # bias for output neurons y1, y2 (same in example)\n",
        "\n",
        "# Targets (desired outputs)\n",
        "T1 = 0.06\n",
        "T2 = 0.88\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.007\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Activation functions\n",
        "# -------------------------------\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative_from_activation(a):\n",
        "    \"\"\"Derivative of sigmoid given activation a = sigmoid(z): a*(1-a).\"\"\"\n",
        "    return a * (1.0 - a)"
      ],
      "metadata": {
        "id": "zuF_eteR5bqF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(x1, x2, w1, w2, w3, w4, w5, w6, w7, w8, b1, b2):\n",
        "    \"\"\"\n",
        "    Compute net inputs and activations for hidden and output layers.\n",
        "    Returns a dict with nets and activations for printing/teaching.\n",
        "    \"\"\"\n",
        "    # Hidden layer linear combinations (net inputs)\n",
        "    H1_net = x1 * w1 + x2 * w2 + b1\n",
        "    H2_net = x1 * w3 + x2 * w4 + b1\n",
        "\n",
        "    # Hidden activations (sigmoid)\n",
        "    H1 = sigmoid(H1_net)\n",
        "    H2 = sigmoid(H2_net)\n",
        "\n",
        "    # Output layer linear combinations\n",
        "    y1_net = H1 * w5 + H2 * w6 + b2\n",
        "    y2_net = H1 * w7 + H2 * w8 + b2\n",
        "\n",
        "    # Output activations (sigmoid)\n",
        "    y1 = sigmoid(y1_net)\n",
        "    y2 = sigmoid(y2_net)\n",
        "\n",
        "    return {\n",
        "        \"H1_net\": H1_net, \"H2_net\": H2_net,\n",
        "        \"H1\": H1, \"H2\": H2,\n",
        "        \"y1_net\": y1_net, \"y2_net\": y2_net,\n",
        "        \"y1\": y1, \"y2\": y2\n",
        "    }\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Forward before update (print step-by-step)\n",
        "# -------------------------------\n",
        "\n",
        "print(\"\\n=== FORWARD PASS (before weight update) ===\")\n",
        "out = forward_pass(x1, x2, w1, w2, w3, w4, w5, w6, w7, w8, b1, b2)\n",
        "\n",
        "print(f\"H1_net = {out['H1_net']:.7f} => H1 = sigmoid(H1_net) = {out['H1']:.9f}\")\n",
        "print(f\"H2_net = {out['H2_net']:.7f} => H2 = sigmoid(H2_net) = {out['H2']:.9f}\")\n",
        "print(f\"y1_net = {out['y1_net']:.9f} => y1 = sigmoid(y1_net) = {out['y1']:.9f}\")\n",
        "print(f\"y2_net = {out['y2_net']:.9f} => y2 = sigmoid(y2_net) = {out['y2']:.9f}\")\n",
        "\n",
        "# Compute per-output squared errors and total error\n",
        "E1 = 0.5 * (T1 - out['y1'])**2\n",
        "E2 = 0.5 * (T2 - out['y2'])**2\n",
        "E_total = E1 + E2\n",
        "print(f\"\\nE1 = 0.5*(T1 - y1)^2 = {E1:.9f}\")\n",
        "print(f\"E2 = 0.5*(T2 - y2)^2 = {E2:.9f}\")\n",
        "print(f\"Total error E_total = E1 + E2 = {E_total:.9f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5) BACKPROPAGATION — output layer\n",
        "#    compute deltas and gradients for w5..w8\n",
        "# -------------------------------\n",
        "\n",
        "print(\"\\n=== BACKPROP: output layer ===\")\n",
        "\n",
        "# For each output neuron i: delta_i = dE/dy_i * dy_i/dnet_i\n",
        "# where dE/dy_i = -(T_i - y_i) for E = 1/2*(T-y)^2\n",
        "dE_dy1 = -(T1 - out['y1'])\n",
        "dy1_dnet = sigmoid_derivative_from_activation(out['y1'])\n",
        "delta1 = dE_dy1 * dy1_dnet  # scalar\n",
        "\n",
        "dE_dy2 = -(T2 - out['y2'])\n",
        "dy2_dnet = sigmoid_derivative_from_activation(out['y2'])\n",
        "delta2 = dE_dy2 * dy2_dnet  # scalar\n",
        "\n",
        "print(f\"dE/dy1 = {dE_dy1:.9f}, dy1/dnet = {dy1_dnet:.9f}, => delta1 = {delta1:.9f}\")\n",
        "print(f\"dE/dy2 = {dE_dy2:.9f}, dy2/dnet = {dy2_dnet:.9f}, => delta2 = {delta2:.9f}\")\n",
        "\n",
        "# Gradients for weights from hidden -> outputs:\n",
        "# dw5 = dE/dw5 = delta1 * H1\n",
        "# dw6 = dE/dw6 = delta1 * H2\n",
        "# dw7 = dE/dw7 = delta2 * H1\n",
        "# dw8 = dE/dw8 = delta2 * H2\n",
        "dw5 = delta1 * out['H1']\n",
        "dw6 = delta1 * out['H2']\n",
        "dw7 = delta2 * out['H1']\n",
        "dw8 = delta2 * out['H2']\n",
        "\n",
        "print(\"\\nGradients for hidden->output weights:\")\n",
        "print(f\"dw5 (for w5) = delta1 * H1 = {dw5:.9f}\")\n",
        "print(f\"dw6 (for w6) = delta1 * H2 = {dw6:.9f}\")\n",
        "print(f\"dw7 (for w7) = delta2 * H1 = {dw7:.9f}\")\n",
        "print(f\"dw8 (for w8) = delta2 * H2 = {dw8:.9f}\")\n",
        "\n",
        "# Update output weights (gradient descent): w <- w - lr * dw\n",
        "w5_new = w5 - lr * dw5\n",
        "w6_new = w6 - lr * dw6\n",
        "w7_new = w7 - lr * dw7\n",
        "w8_new = w8 - lr * dw8\n",
        "\n",
        "print(\"\\nUpdated hidden->output weights (one step):\")\n",
        "print(f\"w5 -> {w5_new:.9f}\")\n",
        "print(f\"w6 -> {w6_new:.9f}\")\n",
        "print(f\"w7 -> {w7_new:.9f}\")\n",
        "print(f\"w8 -> {w8_new:.9f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6) BACKPROPAGATION — hidden layer\n",
        "#    compute deltas for H1, H2 and gradients for w1..w4\n",
        "# -------------------------------\n",
        "\n",
        "print(\"\\n=== BACKPROP: hidden layer ===\")\n",
        "\n",
        "# Error contribution from both output neurons flows back to each hidden neuron:\n",
        "# delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1_net)\n",
        "# delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2_net)\n",
        "# Note: use the original w5,w6,w7,w8 (the ones that were used in the forward pass)\n",
        "delta_H1 = (delta1 * w5 + delta2 * w7) * sigmoid_derivative_from_activation(out['H1'])\n",
        "delta_H2 = (delta1 * w6 + delta2 * w8) * sigmoid_derivative_from_activation(out['H2'])\n",
        "\n",
        "print(f\"delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1) = {delta_H1:.12f}\")\n",
        "print(f\"delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2) = {delta_H2:.12f}\")\n",
        "\n",
        "# Gradients for input->hidden weights:\n",
        "# dw1 = delta_H1 * x1, dw2 = delta_H1 * x2\n",
        "# dw3 = delta_H2 * x1, dw4 = delta_H2 * x2\n",
        "dw1 = delta_H1 * x1\n",
        "dw2 = delta_H1 * x2\n",
        "dw3 = delta_H2 * x1\n",
        "dw4 = delta_H2 * x2\n",
        "\n",
        "print(\"\\nGradients for input->hidden weights:\")\n",
        "print(f\"dw1 (for w1) = delta_H1 * x1 = {dw1:.12f}\")\n",
        "print(f\"dw2 (for w2) = delta_H1 * x2 = {dw2:.12f}\")\n",
        "print(f\"dw3 (for w3) = delta_H2 * x1 = {dw3:.12f}\")\n",
        "print(f\"dw4 (for w4) = delta_H2 * x2 = {dw4:.12f}\")\n",
        "\n",
        "# Update hidden weights\n",
        "w1_new = w1 - lr * dw1\n",
        "w2_new = w2 - lr * dw2\n",
        "w3_new = w3 - lr * dw3\n",
        "w4_new = w4 - lr * dw4\n",
        "\n",
        "print(\"\\nUpdated input->hidden weights (one step):\")\n",
        "print(f\"w1 -> {w1_new:.9f}\")\n",
        "print(f\"w2 -> {w2_new:.9f}\")\n",
        "print(f\"w3 -> {w3_new:.9f}\")\n",
        "print(f\"w4 -> {w4_new:.9f}\")\n",
        "\n",
        "# For completeness, update biases too (if you want)\n",
        "# bias at output b2 <- b2 - lr * delta (sum of deltas for outputs)\n",
        "# bias at hidden b1 <- b1 - lr * delta_H (sum of deltas for hidden neurons)\n",
        "b2_new = b2 - lr * (delta1 + delta2)  # update using both output deltas\n",
        "b1_new = b1 - lr * (delta_H1 + delta_H2)  # update using hidden deltas\n",
        "\n",
        "print(f\"\\nUpdated biases:\")\n",
        "print(f\"b1 -> {b1_new:.9f}\")\n",
        "print(f\"b2 -> {b2_new:.9f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7) Forward pass after the update (to show error decreased)\n",
        "# -------------------------------\n",
        "\n",
        "print(\"\\n=== FORWARD PASS (after weight update) ===\")\n",
        "out_after = forward_pass(x1, x2,\n",
        "                         w1_new, w2_new, w3_new, w4_new,\n",
        "                         w5_new, w6_new, w7_new, w8_new,\n",
        "                         b1_new, b2_new)\n",
        "\n",
        "print(f\"H1 (after) = {out_after['H1']:.9f}\")\n",
        "print(f\"H2 (after) = {out_after['H2']:.9f}\")\n",
        "print(f\"y1 (after) = {out_after['y1']:.9f}\")\n",
        "print(f\"y2 (after) = {out_after['y2']:.9f}\")\n",
        "\n",
        "E1_after = 0.5 * (T1 - out_after['y1'])**2\n",
        "E2_after = 0.5 * (T2 - out_after['y2'])**2\n",
        "E_total_after = E1_after + E2_after\n",
        "\n",
        "print(f\"\\nE_total (before) = {E_total:.9f}\")\n",
        "print(f\"E_total (after)  = {E_total_after:.9f}\")\n",
        "print(\"\\n(You should see the total error decreased after one backprop step.)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksF6Tywp_JzV",
        "outputId": "047ccd6c-e7d1-4f5e-8c85-587223629824"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FORWARD PASS (before weight update) ===\n",
            "H1_net = 0.2700000 => H1 = sigmoid(H1_net) = 0.567092905\n",
            "H2_net = 0.7850000 => H2 = sigmoid(H2_net) = 0.686756727\n",
            "y1_net = 0.569168398 => y1 = sigmoid(y1_net) = 0.638571265\n",
            "y2_net = 0.785288226 => y2 = sigmoid(y2_net) = 0.686818727\n",
            "\n",
            "E1 = 0.5*(T1 - y1)^2 = 0.167372355\n",
            "E2 = 0.5*(T2 - y2)^2 = 0.018659502\n",
            "Total error E_total = E1 + E2 = 0.186031857\n",
            "\n",
            "=== BACKPROP: output layer ===\n",
            "dE/dy1 = 0.578571265, dy1/dnet = 0.230798004, => delta1 = 0.133533093\n",
            "dE/dy2 = -0.193181273, dy2/dnet = 0.215098763, => delta2 = -0.041553053\n",
            "\n",
            "Gradients for hidden->output weights:\n",
            "dw5 (for w5) = delta1 * H1 = 0.075725670\n",
            "dw6 (for w6) = delta1 * H2 = 0.091704750\n",
            "dw7 (for w7) = delta2 * H1 = -0.023564441\n",
            "dw8 (for w8) = delta2 * H2 = -0.028536839\n",
            "\n",
            "Updated hidden->output weights (one step):\n",
            "w5 -> 0.299469920\n",
            "w6 -> 0.449358067\n",
            "w7 -> 0.560164951\n",
            "w8 -> 0.550199758\n",
            "\n",
            "=== BACKPROP: hidden layer ===\n",
            "delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1) = 0.004121974146\n",
            "delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2) = 0.008010218257\n",
            "\n",
            "Gradients for input->hidden weights:\n",
            "dw1 (for w1) = delta_H1 * x1 = 0.002060987073\n",
            "dw2 (for w2) = delta_H1 * x2 = 0.003297579317\n",
            "dw3 (for w3) = delta_H2 * x1 = 0.004005109128\n",
            "dw4 (for w4) = delta_H2 * x2 = 0.006408174605\n",
            "\n",
            "Updated input->hidden weights (one step):\n",
            "w1 -> 0.019985573\n",
            "w2 -> 0.199976917\n",
            "w3 -> 0.249971964\n",
            "w4 -> 0.699955143\n",
            "\n",
            "Updated biases:\n",
            "b1 -> 0.099915075\n",
            "b2 -> 0.089356140\n",
            "\n",
            "=== FORWARD PASS (after weight update) ===\n",
            "H1 (after) = 0.567065751\n",
            "H2 (after) = 0.686727721\n",
            "y1 (after) = 0.638246589\n",
            "y2 (after) = 0.686723150\n",
            "\n",
            "E_total (before) = 0.186031857\n",
            "E_total (after)  = 0.185862529\n",
            "\n",
            "(You should see the total error decreased after one backprop step.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Softmax Activation Function***"
      ],
      "metadata": {
        "id": "-yfh3JuoARko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# sample inputs & targets\n",
        "x1, x2 = 0.5, 0.8\n",
        "T1, T2 = 1, 0        # class 1 is the correct class\n",
        "\n",
        "# initial weights\n",
        "w1=w2=w3=w4=w5=w6=w7=w8=0.5\n",
        "b1=b2=0.1\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "train_step_softmax(\n",
        "    x1, x2, T1, T2,\n",
        "    w1, w2, w3, w4,\n",
        "    w5, w6, w7, w8,\n",
        "    b1, b2, lr\n",
        ")\n",
        "\n",
        "def train_step_softmax(x1, x2, T1, T2,\n",
        "                       w1, w2, w3, w4,\n",
        "                       w5, w6, w7, w8,\n",
        "                       b1, b2, lr):\n",
        "\n",
        "    # ---------------- FORWARD PASS -------------------\n",
        "    out = forward_pass_softmax(\n",
        "        x1, x2,\n",
        "        w1, w2, w3, w4,\n",
        "        w5, w6, w7, w8,\n",
        "        b1, b2\n",
        "    )\n",
        "\n",
        "    print(\"\\n=========== FORWARD PASS (Softmax) ===========\")\n",
        "    print(f\"H1 = {out['H1']:.6f}, H2 = {out['H2']:.6f}\")\n",
        "    print(f\"y1 = {out['y1']:.6f}, y2 = {out['y2']:.6f}\")\n",
        "\n",
        "    # Errors BEFORE update\n",
        "    E_before = 0.5 * ((T1 - out[\"y1\"])**2 + (T2 - out[\"y2\"])**2)\n",
        "    print(f\"Total Error (BEFORE) = {E_before:.9f}\")\n",
        "\n",
        "    # ---------- Softmax Backprop (with MSE) ----------\n",
        "    y1, y2 = out[\"y1\"], out[\"y2\"]\n",
        "    Y = np.array([y1, y2])\n",
        "    T = np.array([T1, T2])\n",
        "\n",
        "    dE_dy = -(T - Y)\n",
        "\n",
        "    # Softmax Jacobian\n",
        "    J = np.array([\n",
        "        [y1*(1-y1), -y1*y2],\n",
        "        [-y1*y2,   y2*(1-y2)]\n",
        "    ])\n",
        "\n",
        "    delta = J @ dE_dy\n",
        "    delta1, delta2 = delta\n",
        "\n",
        "    print(\"\\n=========== OUTPUT LAYER DELTAS ===========\")\n",
        "    print(f\"delta1 = {delta1:.9f}\")\n",
        "    print(f\"delta2 = {delta2:.9f}\")\n",
        "\n",
        "    # ----------- GRADIENTS hidden→output -----------\n",
        "    H1 = out[\"H1\"]\n",
        "    H2 = out[\"H2\"]\n",
        "\n",
        "    dw5 = delta1 * H1\n",
        "    dw6 = delta1 * H2\n",
        "    dw7 = delta2 * H1\n",
        "    dw8 = delta2 * H2\n",
        "\n",
        "    w5_new = w5 - lr * dw5\n",
        "    w6_new = w6 - lr * dw6\n",
        "    w7_new = w7 - lr * dw7\n",
        "    w8_new = w8 - lr * dw8\n",
        "\n",
        "    # ----------- BACKPROP TO HIDDEN LAYER -----------\n",
        "    delta_H1 = (delta1*w5 + delta2*w7) * sigmoid_derivative_from_activation(H1)\n",
        "    delta_H2 = (delta1*w6 + delta2*w8) * sigmoid_derivative_from_activation(H2)\n",
        "\n",
        "    print(\"\\n=========== HIDDEN LAYER DELTAS ===========\")\n",
        "    print(f\"delta_H1 = {delta_H1:.9f}\")\n",
        "    print(f\"delta_H2 = {delta_H2:.9f}\")\n",
        "\n",
        "    dw1 = delta_H1 * x1\n",
        "    dw2 = delta_H1 * x2\n",
        "    dw3 = delta_H2 * x1\n",
        "    dw4 = delta_H2 * x2\n",
        "\n",
        "    w1_new = w1 - lr * dw1\n",
        "    w2_new = w2 - lr * dw2\n",
        "    w3_new = w3 - lr * dw3\n",
        "    w4_new = w4 - lr * dw4\n",
        "\n",
        "    b1_new = b1 - lr*(delta_H1 + delta_H2)\n",
        "    b2_new = b2 - lr*(delta1 + delta2)\n",
        "\n",
        "    print(\"\\n=========== UPDATED WEIGHTS ===========\")\n",
        "    print(f\"w1={w1_new:.6f}, w2={w2_new:.6f}, w3={w3_new:.6f}, w4={w4_new:.6f}\")\n",
        "    print(f\"w5={w5_new:.6f}, w6={w6_new:.6f}, w7={w7_new:.6f}, w8={w8_new:.6f}\")\n",
        "    print(f\"b1={b1_new:.6f}, b2={b2_new:.6f}\")\n",
        "\n",
        "    # ---------- RUN FORWARD PASS AGAIN TO SEE NEW ERROR ----------\n",
        "    out_new = forward_pass_softmax(\n",
        "        x1, x2,\n",
        "        w1_new, w2_new, w3_new, w4_new,\n",
        "        w5_new, w6_new, w7_new, w8_new,\n",
        "        b1_new, b2_new\n",
        "    )\n",
        "\n",
        "    E_after = 0.5 * ((T1 - out_new[\"y1\"])**2 + (T2 - out_new[\"y2\"])**2)\n",
        "    print(f\"\\n=========== TOTAL ERROR (AFTER UPDATE) ===========\")\n",
        "    print(f\"Total Error (AFTER) = {E_after:.9f}\")\n",
        "\n",
        "    return (w1_new, w2_new, w3_new, w4_new,\n",
        "            w5_new, w6_new, w7_new, w8_new,\n",
        "            b1_new, b2_new)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uIK78Bf5rAf",
        "outputId": "72535e20-d7c5-449c-ecb4-0a75165eb79c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========== FORWARD PASS (Softmax) ===========\n",
            "H1 = 0.679179, H2 = 0.679179\n",
            "y1 = 0.500000, y2 = 0.500000\n",
            "Total Error (BEFORE) = 0.250000000\n",
            "\n",
            "=========== OUTPUT LAYER DELTAS ===========\n",
            "delta1 = -0.250000000\n",
            "delta2 = 0.250000000\n",
            "\n",
            "=========== HIDDEN LAYER DELTAS ===========\n",
            "delta_H1 = 0.000000000\n",
            "delta_H2 = 0.000000000\n",
            "\n",
            "=========== UPDATED WEIGHTS ===========\n",
            "w1=0.500000, w2=0.500000, w3=0.500000, w4=0.500000\n",
            "w5=0.516979, w6=0.516979, w7=0.483021, w8=0.483021\n",
            "b1=0.100000, b2=0.100000\n",
            "\n",
            "=========== TOTAL ERROR (AFTER UPDATE) ===========\n",
            "Total Error (AFTER) = 0.238602894\n"
          ]
        }
      ]
    }
  ]
}